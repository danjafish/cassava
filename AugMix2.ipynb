{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5,6,7'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CassvaImgClassifier\n",
    "import torch.nn.functional as F\n",
    "from utils import seed_everything\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "import albumentations\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import apex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'test_fold': 0,\n",
    "    'seed': 2021,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'log_file': \"/home/samenko/Cassava/logs/augmix_cosLR_Parallel4_Adam_albAugs.log\",\n",
    "    'img_size': 512,\n",
    "    'epochs': 20,\n",
    "    'train_bs': 8*4,\n",
    "    'valid_bs': 32,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-4,#0.1,#\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-6,\n",
    "    'num_workers': 8,\n",
    "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'fp16': True,\n",
    "    'print_freq':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/data/Cassava/train.csv')\n",
    "data['fold'] = 0\n",
    "strkf = StratifiedKFold(n_splits=5)\n",
    "_ = strkf.get_n_splits(data.image_id, data.label)\n",
    "f = 0\n",
    "for train_index, test_index in strkf.split(data.image_id, data.label):\n",
    "    data.loc[data.index.isin(test_index), 'fold'] = f\n",
    "    f = f + 1\n",
    "\n",
    "train_data = data[(data.fold != CFG['test_fold'])].reset_index(drop=True)\n",
    "val_data = data[data.fold == CFG['test_fold']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb\n",
    "    #return Image.open(path)#.thumbnail((512,512,3), Image.ANTIALIAS)\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, transforms):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self, ):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[self.df.index == idx]\n",
    "        image_name = row.image_id.values[0]\n",
    "        img = get_img('/home/data/Cassava/train_images/' + image_name)\n",
    "        #img = self.transforms(img)\n",
    "        img = self.transforms(image=img)['image']\n",
    "        img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        label = row.label.values[0]\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUGS = Compose([\n",
    "    RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "    Transpose(p=0.5),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    ShiftScaleRotate(p=0.5),\n",
    "    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "    RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    CoarseDropout(p=0.5),\n",
    "    Cutout(p=0.5),\n",
    "    #ToTensorV2(p=1.0),\n",
    "], p=1.)\n",
    "\n",
    "\n",
    "TEST_AUGS = Compose([\n",
    "    CenterCrop(CFG['img_size'], CFG['img_size'], p=1.0),\n",
    "    Resize(CFG['img_size'], CFG['img_size']),\n",
    "    #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    #ToTensorV2(p=1.0),\n",
    "], p=1.)\n",
    "\n",
    "val_ds = CassavaDataset(val_data, TRAIN_AUGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import augmentations\n",
    "class AugMixDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset wrapper to perform AugMix augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, preprocess, no_jsd=False, val_mode = False):\n",
    "        self.dataset = dataset\n",
    "        self.preprocess = preprocess\n",
    "        self.no_jsd = no_jsd\n",
    "        self.val_mode = val_mode\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.dataset[i]\n",
    "        if self.val_mode:\n",
    "            return self.preprocess(x), y\n",
    "        if self.no_jsd:\n",
    "            return aug(x, self.preprocess), y\n",
    "        else:\n",
    "            im_tuple = (self.preprocess(x), aug(x, self.preprocess),\n",
    "                        aug(x, self.preprocess))\n",
    "            return im_tuple, y\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def aug(image, preprocess):\n",
    "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
    "    Args:\n",
    "      image: PIL.Image input image\n",
    "      preprocess: Preprocessing function which should return a torch tensor.\n",
    "    Returns:\n",
    "      mixed: Augmented and mixed image.\n",
    "    \"\"\"\n",
    "    aug_list = augmentations.augmentations\n",
    "    if args['all_ops']:\n",
    "        aug_list = augmentations.augmentations_all\n",
    "\n",
    "    ws = np.float32(np.random.dirichlet([1] * args['mixture_width']))\n",
    "    m = np.float32(np.random.beta(1, 1))\n",
    "\n",
    "    mix = torch.zeros_like(preprocess(image))\n",
    "    for i in range(args['mixture_width']):\n",
    "        image_aug = image.copy()\n",
    "        depth = args['mixture_depth'] if args['mixture_depth'] > 0 else np.random.randint(\n",
    "            1, 4)\n",
    "        for _ in range(depth):\n",
    "            op = np.random.choice(aug_list)\n",
    "            image_aug = op(image_aug, args['aug_severity'])\n",
    "        # Preprocessing commutes since all coefficients are convex\n",
    "        mix += ws[i] * preprocess(image_aug)\n",
    "\n",
    "    mixed = (1 - m) * preprocess(image) + m * mix\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "args ={\"all_ops\":True,\n",
    "      \"mixture_width\":3,\n",
    "      \"mixture_depth\":np.random.randint(1, 4),\n",
    "       \"aug_severity\":3\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = CassavaDataset(train_data, TRAIN_AUGS)\n",
    "val_ds = CassavaDataset(val_data, TEST_AUGS)\n",
    "am_train_data = AugMixDataset(train_ds, preprocess, no_jsd=False)\n",
    "am_val_data = AugMixDataset(val_ds, preprocess, no_jsd=False, val_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = CassavaDataset(train_data, train_transform)\n",
    "# val_ds = CassavaDataset(val_data, val_transform)\n",
    "# am_train_data = AugMixDataset(train_ds, preprocess, no_jsd=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    am_train_data,\n",
    "    batch_size=CFG['train_bs'],\n",
    "    shuffle=True,\n",
    "    num_workers=CFG['num_workers'],\n",
    "    pin_memory=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    am_val_data, #val_ds,\n",
    "    batch_size=CFG['valid_bs'],\n",
    "    shuffle=False,\n",
    "    num_workers=CFG['num_workers'],\n",
    "    pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "model = CassvaImgClassifier(CFG['model_arch'], data.label.nunique(), pretrained=True)#.to(device)\n",
    "model = torch.nn.DataParallel(model).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
    "\n",
    "# model, optimizer = apex.amp.initialize(\n",
    "#                 model,\n",
    "#                 optimizer,\n",
    "#                 opt_level='O1')\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['epochs'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
    "loss_val = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch( model, val_loader, loss_fn, epoch):\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        model = model.eval();\n",
    "        for step, (x, y_true) in pbar:\n",
    "            x = x.to(device).float()\n",
    "            y_true = y_true.to(device).long()\n",
    "            y_pred = model(x)\n",
    "            preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n",
    "            targets_all += [y_true.detach().cpu().numpy()]\n",
    "            l = loss_fn(y_pred, y_true)\n",
    "            loss_sum += l.item() * y_true.shape[0]\n",
    "            sample_num += y_true.shape[0]\n",
    "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'val epoch {epoch} loss: {loss_sum / sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    targets_all = np.concatenate(targets_all)\n",
    "    print('validation multi-class accuracy = {:.4f}'.format((preds_all == targets_all).mean()))\n",
    "    with open(CFG['log_file'], 'a+') as logger:\n",
    "        logger.write(f\"Epoch: {epoch} val acc = {(preds_all == targets_all).mean()}\\n\")\n",
    "    #return (preds_all == targets_all).mean(), loss_sum / sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cm/shared/apps/jupyterhub/0.8.1/lib/python3.6/site-packages/ipykernel_launcher.py:5: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa7d4d977ef4332925a0d3feb6749e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=535.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in range(CFG['epochs']):\n",
    "    \n",
    "    model = model.train()\n",
    "    loss_ema = 0.\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True)\n",
    "    for step, (images, targets) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        #augmix\n",
    "        images_all = torch.cat(images, 0).cuda()\n",
    "        targets = targets.cuda()\n",
    "        logits_all = model(images_all)\n",
    "        logits_clean, logits_aug1, logits_aug2 = torch.split(logits_all, images[0].size(0))\n",
    "        loss = F.cross_entropy(logits_clean, targets)\n",
    "        p_clean, p_aug1, p_aug2 = F.softmax(\n",
    "          logits_clean, dim=1), F.softmax(\n",
    "              logits_aug1, dim=1), F.softmax(\n",
    "                  logits_aug2, dim=1)\n",
    "        # Clamp mixture distribution to avoid exploding KL divergence\n",
    "        p_mixture = torch.clamp((p_clean + p_aug1 + p_aug2) / 3., 1e-7, 1).log()\n",
    "        loss += 12 * (F.kl_div(p_mixture, p_clean, reduction='batchmean') +\n",
    "                            F.kl_div(p_mixture, p_aug1, reduction='batchmean') +\n",
    "                            F.kl_div(p_mixture, p_aug2, reduction='batchmean')) / 3.\n",
    "        #with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_ema = loss_ema * 0.9 + float(loss) * 0.1\n",
    "        description = f'tain epoch {epoch} loss_ema: {loss_ema:.4f} loss: {loss:.4f}'\n",
    "        pbar.set_description(description)\n",
    "        #if step % CFG['print_freq'] == 0: print('Train Loss {:.4f}'.format(loss_ema))\n",
    "    \n",
    "    #train_loss_ema = train(model, train_loader, optimizer, scheduler)\n",
    "    valid_one_epoch(model, val_loader, loss_val, epoch)\n",
    "    #model = model.eval()\n",
    "    #total_loss = 0.\n",
    "    #total_correct = 0\n",
    "    #with torch.no_grad():\n",
    "    #    for images, targets in test_loader:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
