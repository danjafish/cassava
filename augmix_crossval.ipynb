{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '4,5,6,7'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from torch.utils.data import Dataset\n",
    "from albumentations import (\n",
    "    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90,\n",
    "    Transpose, ShiftScaleRotate, Blur, OpticalDistortion, GridDistortion, HueSaturationValue,\n",
    "    IAAAdditiveGaussianNoise, GaussNoise, MotionBlur, MedianBlur, IAAPiecewiseAffine, RandomResizedCrop,\n",
    "    IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip, OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n",
    ")\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CassvaImgClassifier\n",
    "import torch.nn.functional as F\n",
    "from utils import seed_everything\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform\n",
    "import albumentations\n",
    "from PIL import Image, ImageOps, ImageEnhance\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import apex\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'test_fold': 0,\n",
    "    'seed': 2021,\n",
    "    'model_arch': 'tf_efficientnet_b4_ns',\n",
    "    'log_file': \"/home/samenko/Cassava/logs/b4_final_cv.log\",\n",
    "    'img_size': 512,\n",
    "    'epochs': 20,\n",
    "    'train_bs': 8*4,\n",
    "    'valid_bs': 32,\n",
    "    'T_0': 10,\n",
    "    'lr': 1e-4,#0.1,#\n",
    "    'min_lr': 1e-6,\n",
    "    'weight_decay':1e-6,\n",
    "    'num_workers': 8*4,\n",
    "    'accum_iter': 2, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n",
    "    'verbose_step': 1,\n",
    "    'device': 'cuda:0',\n",
    "    'fp16': True,\n",
    "    'print_freq':1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0001, 0.001, 0.03, 0.0003, 0.0003, 0.0003)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-4, 1e-3, 3e-2, 3.00e-4, 3e-4, 3.00E-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('/home/data/Cassava/train.csv')\n",
    "# data['fold'] = 0\n",
    "# strkf = StratifiedKFold(n_splits=5)\n",
    "# _ = strkf.get_n_splits(data.image_id, data.label)\n",
    "# f = 0\n",
    "# for train_index, test_index in strkf.split(data.image_id, data.label):\n",
    "#     data.loc[data.index.isin(test_index), 'fold'] = f\n",
    "#     f = f + 1\n",
    "\n",
    "# train_data = data[(data.fold != CFG['test_fold'])].reset_index(drop=True)\n",
    "# val_data = data[data.fold == CFG['test_fold']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#soft_labels = pd.read_csv('../../tmp/soft_labels.csv')\n",
    "#soft_labels = soft_labels.dropna()\n",
    "#soft_labels.reset_index(inplace=True, drop=True)\n",
    "\n",
    "soft_labels = pd.read_csv('../../tmp/effb3_oof_predictions.csv')\n",
    "#soft_labels = pd.read_csv('../../tmp/resnext101_oof_predictions.csv')\n",
    "\n",
    "soft_labels.columns = ['image_id','_target','fold','p0','p1','p2','p3','p4','label','pred_class_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "soft_labels['drop'] = (soft_labels['label'] != soft_labels['_target']) &  (soft_labels['pred_class_proba'] > 0.95)\n",
    "soft_labels['drop'].sum()\n",
    "\n",
    "#train_data = soft_labels[~soft_labels['drop']][['image_id','label','fold']].reset_index(drop=True)\n",
    "clean_soft =soft_labels[~soft_labels['drop']][['image_id','label','fold','p0','p1','p2','p3','p4']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# irrelevant_image_ids = [274726002, 9224019, 159654644, 199112616, 226533928, 262902341, 269713568, 274726002, 384390206, 390601409, 421035788, 457405364, 600736721, 580111608,\n",
    "# 616718743, 695438825, 723564013, 826231979, 847847826, 927165736, 1004389140,\n",
    "# 1008244905, 1338159402, 1339403533, 1359893940, 1366430957, 1689510013, 9224019,\n",
    "# 4269208386, 4239074071, 3810809174, 3652033201, 3609350672, 3609986814,\n",
    "# 3477169212, 3435954655, 3425850136, 3251960666, 3252232501, 3199643560,\n",
    "# 3126296051, 3040241097, 2981404650, 2925605732, 2839068946, 2698282165,\n",
    "# 2604713994, 2415837573, 2382642453, 2321669192, 2320471703, 2278166989,\n",
    "# 2276509518, 2262263316, 2182500020, 2139839273, 2084868828, 1848686439,\n",
    "# 1689510013, 1359893940]\n",
    "\n",
    "# noisy_image_ids = [ 410880003, 411955232, 501215014, 549854027, 554488826,\n",
    "# 724195836, 744383303, 888983519, 1096438409, 1130568730,\n",
    "# 1709404074, 1770746162, 4280523848, 3530560257, 3421208425,\n",
    "# 3321193739, 3086663390, 3045134829, 1862072615]\n",
    "\n",
    "\n",
    "# noisy_image_ids = list(map(lambda x:str(x)+'.jpg',noisy_image_ids))\n",
    "# irrelevant_image_ids = list(map(lambda x:str(x)+'.jpg',irrelevant_image_ids))\n",
    "\n",
    "# for elem in noisy_image_ids:\n",
    "#     soft_labels.loc[soft_labels['image_id']== elem, 'drop']=True\n",
    "# 231+len(noisy_image_ids), soft_labels['drop'].sum()\n",
    "\n",
    "# for elem in irrelevant_image_ids:\n",
    "#     soft_labels.loc[soft_labels['image_id']== elem, 'drop']=True\n",
    "# 246+len(irrelevant_image_ids), soft_labels['drop'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = soft_labels[~soft_labels['drop']][['image_id','label','fold']].reset_index(drop=True)\n",
    "# train_data['label'] = train_data['label'].apply(int)\n",
    "# train_data['fold']= train_data['fold'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    im_bgr = cv2.imread(path)\n",
    "    im_rgb = im_bgr[:, :, ::-1]\n",
    "    return im_rgb\n",
    "    #return Image.open(path)#.thumbnail((512,512,3), Image.ANTIALIAS)\n",
    "\n",
    "class CassavaDataset(Dataset):\n",
    "    def __init__(self, df, transforms, mode = None):\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.transforms = transforms\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self, ):\n",
    "        \n",
    "        return len(self.df)\n",
    "    def to_one_hot(self, le_label, num_classes = 5):\n",
    "        oho_label = np.zeros(num_classes)\n",
    "        oho_label[int(le_label)] = 1\n",
    "        return oho_label\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df[self.df.index == idx]\n",
    "        label = row.label.values[0]\n",
    "        image_name = row.image_id.values[0]\n",
    "        img = get_img('/home/data/Cassava/train_images/' + image_name)\n",
    "        #img = self.transforms(img)\n",
    "        img = self.transforms(image=img)['image']\n",
    "        img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "        if self.mode == 'soft':\n",
    "            label = self.to_one_hot(label)\n",
    "            soft_labels = row.values[0][3:]\n",
    "            label = (label * 0.7).astype(np.float16) + (soft_labels * 0.3).astype(np.float16)\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUGS = Compose([\n",
    "    RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n",
    "    Transpose(p=0.5),\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    ShiftScaleRotate(scale_limit=0.2, p=0.5),\n",
    "    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "    RandomBrightnessContrast(brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n",
    "    #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    CoarseDropout(max_holes=12, max_height=int(0.11*512), max_width=int(0.11*512), min_holes=1, min_height=int(0.03*512), min_width=int(0.03*512), p=0.5),\n",
    "    Cutout(p=0.5),\n",
    "    #ToTensorV2(p=1.0),\n",
    "], p=1.)\n",
    "\n",
    "\n",
    "TEST_AUGS = Compose([\n",
    "    CenterCrop(CFG['img_size'], CFG['img_size'], p=1.0),\n",
    "    Resize(CFG['img_size'], CFG['img_size']),\n",
    "    #Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n",
    "    #ToTensorV2(p=1.0),\n",
    "], p=1.)\n",
    "\n",
    "# val_ds = CassavaDataset(val_data, TRAIN_AUGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossOneHot(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossOneHot, self).__init__()\n",
    "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, preds, labels):\n",
    "        return torch.mean(torch.sum(-labels * self.log_softmax(preds), -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import augmentations\n",
    "class AugMixDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Dataset wrapper to perform AugMix augmentation.\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, preprocess, no_jsd=False, val_mode = False):\n",
    "        self.dataset = dataset\n",
    "        self.preprocess = preprocess\n",
    "        self.no_jsd = no_jsd\n",
    "        self.val_mode = val_mode\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x, y = self.dataset[i]\n",
    "        if self.val_mode:\n",
    "            return self.preprocess(x), y\n",
    "        if self.no_jsd:\n",
    "            return aug(x, self.preprocess), y\n",
    "        else:\n",
    "            im_tuple = (self.preprocess(x), aug(x, self.preprocess),\n",
    "                        aug(x, self.preprocess))\n",
    "            return im_tuple, y\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def aug(image, preprocess):\n",
    "    \"\"\"Perform AugMix augmentations and compute mixture.\n",
    "    Args:\n",
    "      image: PIL.Image input image\n",
    "      preprocess: Preprocessing function which should return a torch tensor.\n",
    "    Returns:\n",
    "      mixed: Augmented and mixed image.\n",
    "    \"\"\"\n",
    "    aug_list = augmentations.augmentations\n",
    "    if args['all_ops']:\n",
    "        aug_list = augmentations.augmentations_all\n",
    "\n",
    "    ws = np.float32(np.random.dirichlet([1] * args['mixture_width']))\n",
    "    m = np.float32(np.random.beta(1, 1))\n",
    "\n",
    "    mix = torch.zeros_like(preprocess(image))\n",
    "    for i in range(args['mixture_width']):\n",
    "        image_aug = image.copy()\n",
    "        depth = args['mixture_depth'] if args['mixture_depth'] > 0 else np.random.randint(\n",
    "            1, 4)\n",
    "        for _ in range(depth):\n",
    "            op = np.random.choice(aug_list)\n",
    "            image_aug = op(image_aug, args['aug_severity'])\n",
    "        # Preprocessing commutes since all coefficients are convex\n",
    "        mix += ws[i] * preprocess(image_aug)\n",
    "\n",
    "    mixed = (1 - m) * preprocess(image) + m * mix\n",
    "    return mixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "args ={\"all_ops\":True,\n",
    "      \"mixture_width\":3,\n",
    "      \"mixture_depth\":np.random.randint(1, 4),\n",
    "       \"aug_severity\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, train_loader, loss_fn, epoch):\n",
    "    model = model.train()\n",
    "    loss_ema = 0.\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader), position=0, leave=True)\n",
    "    for step, (images, targets) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        #augmix\n",
    "        images_all = torch.cat(images, 0).cuda()\n",
    "        targets = targets.cuda()\n",
    "        logits_all = model(images_all)\n",
    "        logits_clean, logits_aug1, logits_aug2 = torch.split(logits_all, images[0].size(0))\n",
    "        #loss = F.cross_entropy(logits_clean, targets.long())\n",
    "        loss = loss_fn(logits_clean, targets)\n",
    "        p_clean, p_aug1, p_aug2 = F.softmax(\n",
    "          logits_clean, dim=1), F.softmax(\n",
    "              logits_aug1, dim=1), F.softmax(\n",
    "                  logits_aug2, dim=1)\n",
    "        # Clamp mixture distribution to avoid exploding KL divergence\n",
    "        p_mixture = torch.clamp((p_clean + p_aug1 + p_aug2) / 3., 1e-7, 1).log()\n",
    "        loss += 12 * (F.kl_div(p_mixture, p_clean, reduction='batchmean') +\n",
    "                            F.kl_div(p_mixture, p_aug1, reduction='batchmean') +\n",
    "                            F.kl_div(p_mixture, p_aug2, reduction='batchmean')) / 3.\n",
    "        #with apex.amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        #    scaled_loss.backward()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        loss_ema = loss_ema * 0.9 + float(loss) * 0.1\n",
    "        description = f'tain epoch {epoch} loss_ema: {loss_ema:.4f} loss: {loss:.4f}'\n",
    "        pbar.set_description(description)\n",
    "        #if step % CFG['print_freq'] == 0: print('Train Loss {:.4f}'.format(loss_ema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_one_epoch( model, val_loader, loss_fn, epoch, dev_fold):\n",
    "    loss_sum = 0\n",
    "    sample_num = 0\n",
    "    preds_all = []\n",
    "    targets_all = []\n",
    "    pbar = tqdm(enumerate(val_loader), total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        model = model.eval();\n",
    "        for step, (x, y_true) in pbar:\n",
    "            x = x.to(device).float()\n",
    "            y_true = y_true.to(device).long()\n",
    "            y_pred = model(x)\n",
    "            preds_all += [torch.argmax(y_pred, 1).detach().cpu().numpy()]\n",
    "            targets_all += [y_true.detach().cpu().numpy()]\n",
    "            l = loss_fn(y_pred, y_true)\n",
    "            loss_sum += l.item() * y_true.shape[0]\n",
    "            sample_num += y_true.shape[0]\n",
    "        if ((step + 1) % CFG['verbose_step'] == 0) or ((step + 1) == len(val_loader)):\n",
    "            description = f'val epoch {epoch} loss: {loss_sum / sample_num:.4f}'\n",
    "            pbar.set_description(description)\n",
    "    preds_all = np.concatenate(preds_all)\n",
    "    targets_all = np.concatenate(targets_all)\n",
    "    print('validation multi-class accuracy = {:.4f}'.format((preds_all == targets_all).mean()))\n",
    "    with open(CFG['log_file'], 'a+') as logger:\n",
    "        logger.write(f\"Epoch: {epoch} fold: {dev_fold} val acc = {(preds_all == targets_all).mean()} val loss = {loss_sum / sample_num}\\n\")\n",
    "    #return (preds_all == targets_all).mean(), loss_sum / sample_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev fold: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cm/shared/apps/jupyterhub/0.8.1/lib/python3.6/site-packages/ipykernel_launcher.py:4: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09db642ecf89499b8920e53b059bfda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cm/shared/apps/jupyterhub/0.8.1/lib/python3.6/site-packages/ipykernel_launcher.py:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36d74f627ef43c0a31020069bb29d03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.8851\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8b5d71a849947d19f847a1bd72cf4e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fb7ab678fcc452a8a634d5e5db3059d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2911ff61bb6646c68e621f333ee39ad3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b72c254b2b254cc0a4045cd15365bad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9db290f0e8e437ba6e6093aafdf1111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5184a27e064ddfb89d8ef06909ea1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031c280ffa8e4c3b98a1a49f9ed9e221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a75ab1abdd4ff1a0d7e642ca0d96d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd3f54e4b3e84c12a1318efe60693407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c34d7c9d739c4f03b0710e8c570af567",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da3a0ec66ab49fb84d6222acedb876b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3de2d1edf39b4e528e9c1a475701cfe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af753f49f82b432884d6c6275ed97246",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a214f53964c4126961c729db6abb05a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=130.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validation multi-class accuracy = 0.9214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a32aaba38f0a4147b56024ddd2e73ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=521.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "for dev_fold in [0,1,2,3,4]:\n",
    "    print(f\"dev fold: {dev_fold}\")\n",
    "    train_data = clean_soft[clean_soft.fold != dev_fold].reset_index(drop=True)\n",
    "    val_data = clean_soft[clean_soft.fold == dev_fold].reset_index(drop=True)    \n",
    "    #train_ds = CassavaDataset(train_data, TRAIN_AUGS)\n",
    "    train_ds = CassavaDataset(train_data, TRAIN_AUGS, mode='soft')\n",
    "    val_ds = CassavaDataset(val_data, TEST_AUGS)\n",
    "    am_train_data = AugMixDataset(train_ds, preprocess, no_jsd=False)\n",
    "    am_val_data = AugMixDataset(val_ds, preprocess, no_jsd=False, val_mode=True)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "    am_train_data,\n",
    "    batch_size=CFG['train_bs'],\n",
    "    shuffle=True,\n",
    "    num_workers=CFG['num_workers'],\n",
    "    pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        am_val_data, #val_ds,\n",
    "        batch_size=CFG['valid_bs'],\n",
    "        shuffle=False,\n",
    "        num_workers=CFG['num_workers'],\n",
    "        pin_memory=True)\n",
    "    \n",
    "    \n",
    "    model = CassvaImgClassifier(CFG['model_arch'], 5, pretrained=True)#.to(device)\n",
    "    model = torch.nn.DataParallel(model).cuda()\n",
    "    cudnn.benchmark = True\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'])\n",
    "\n",
    "    # model, optimizer = apex.amp.initialize(\n",
    "    #                 model,\n",
    "    #                 optimizer,\n",
    "    #                 opt_level='O1')\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=CFG['epochs'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n",
    "    loss_val = nn.CrossEntropyLoss().to(device)\n",
    "    loss_fn = CrossEntropyLossOneHot()\n",
    "    \n",
    "    \n",
    "    for epoch in range(CFG['epochs']):\n",
    "        train_one_epoch(model, optimizer, train_loader, loss_fn, epoch)\n",
    "        #train_loss_ema = train(model, train_loader, optimizer, scheduler)\n",
    "        valid_one_epoch(model, val_loader, loss_val, epoch, dev_fold)\n",
    "        torch.save(model.state_dict(), '/home/samenko/Cassava/output/eff_b4/{}_fold_{}_{}'.format(CFG['model_arch'], dev_fold, epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
